{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehaprasanth918/acoustic-pest-detection/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# 🚀 Enable High-RAM Mode in Colab: Runtime → Change runtime type → High-RAM\n",
        "# 🚀 Mount Google Drive if necessary\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths to the dataset\n",
        "combined_features_path = \"/content/drive/My Drive/dataset/combined_mfcc_features.npy\"\n",
        "combined_labels_path = \"/content/drive/My Drive/dataset/combined_mfcc_labels.npy\"\n",
        "\n",
        "# 🛠 Function to load dataset in batches (Prevents RAM crash)\n",
        "def load_large_npy(file_path, batch_size=1000):\n",
        "    data = np.load(file_path, allow_pickle=True)  # Load entire array\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        yield data[i:i+batch_size]\n",
        "\n",
        "# ✅ Load and Convert to Efficient Data Type\n",
        "X = np.load(combined_features_path, allow_pickle=True).astype(np.float32)  # Convert to float32 to save memory\n",
        "y = np.load(combined_labels_path, allow_pickle=True)\n",
        "\n",
        "print(\"✅ Dataset loaded successfully!\")\n",
        "print(f\"📌 Total samples: {len(X)}\")\n",
        "print(f\"📊 MFCC Feature Shape: {X[0].shape}\")  # Shape of one sample\n",
        "print(f\"📋 Unique Classes: {np.unique(y)}\")\n",
        "\n",
        "# 🔄 Step 1: Compute max_time_steps efficiently (Use 95th percentile to prevent excessive padding)\n",
        "mfcc_lengths = [mfcc.shape[1] for mfcc in X]\n",
        "max_time_steps = int(np.percentile(mfcc_lengths, 95))  # 95th percentile\n",
        "print(f\"🔹 Optimal max_time_steps: {max_time_steps}\")\n",
        "\n",
        "# 🚀 Step 2: Efficiently Pad MFCCs\n",
        "def pad_mfcc(mfcc, target_length):\n",
        "    pad_width = target_length - mfcc.shape[1]\n",
        "    if pad_width > 0:\n",
        "        return np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')  # Pad only time axis\n",
        "    return mfcc[:, :target_length]  # Trim if too long\n",
        "\n",
        "X_padded = np.array([pad_mfcc(mfcc, max_time_steps) for mfcc in X], dtype=np.float32)\n",
        "\n",
        "# 🚀 Step 3: Normalize MFCC values efficiently\n",
        "X_normalized = (X_padded - np.mean(X_padded, axis=(0, 1))) / np.std(X_padded, axis=(0, 1))\n",
        "\n",
        "# 🚀 Step 4: Encode labels as numbers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# 🚀 Step 5: Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# 🔄 Step 6: Reshape for Edge Impulse compatibility (Swap timesteps & features)\n",
        "X_train = np.transpose(X_train, (0, 2, 1))  # Shape: (samples, time_steps, features)\n",
        "X_test = np.transpose(X_test, (0, 2, 1))\n",
        "\n",
        "# 🔥 Garbage Collection to Free Memory\n",
        "gc.collect()\n",
        "\n",
        "# ✅ Final Status\n",
        "print(\"✅ Preprocessing completed!\")\n",
        "print(f\"📌 Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
        "print(f\"🔢 Label encoding: {dict(enumerate(label_encoder.classes_))}\")  # Show class mappings\n",
        "print(f\"✔ Final X_train shape: {X_train.shape}\")  # Expected (samples, time_steps, features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcYwp3_qDKIl",
        "outputId": "3e8f3e66-e5e2-4540-fcdb-21b6c044b6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Dataset loaded successfully!\n",
            "📌 Total samples: 62332\n",
            "📊 MFCC Feature Shape: (13, 79)\n",
            "📋 Unique Classes: ['bombus terrestris' 'bradysia difformis' 'coccilena septempunctata'\n",
            " 'myzus persicae' 'nezara viridula' 'palomena prasina'\n",
            " 'trialeurodes vaporariorum' 'tuta absoluta']\n",
            "🔹 Optimal max_time_steps: 79\n",
            "✅ Preprocessing completed!\n",
            "📌 Training samples: 49865, Testing samples: 12467\n",
            "🔢 Label encoding: {0: 'bombus terrestris', 1: 'bradysia difformis', 2: 'coccilena septempunctata', 3: 'myzus persicae', 4: 'nezara viridula', 5: 'palomena prasina', 6: 'trialeurodes vaporariorum', 7: 'tuta absoluta'}\n",
            "✔ Final X_train shape: (49865, 79, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess dataset (your existing code)\n",
        "# Ensure MFCC shape is (samples, 79, 13)\n",
        "\n",
        "# Step 2: Flatten MFCC features for Edge Impulse\n",
        "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(\"✅ Flattened MFCC Shape for Edge Impulse:\", X_train_flattened.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdDkY3OmFbKg",
        "outputId": "d1ff1289-a150-401f-d4bb-18c5ec6c2c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Flattened MFCC Shape for Edge Impulse: (49865, 1027)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Bidirectional, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# ✅ Improved GRU Model\n",
        "model = Sequential([\n",
        "    Input(shape=(79, 13)),  # Shape based on MFCCs\n",
        "\n",
        "    # 🔥 Bidirectional GRUs\n",
        "    Bidirectional(GRU(256, return_sequences=True, recurrent_dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Bidirectional(GRU(256, return_sequences=True, recurrent_dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.2)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    GRU(64),  # Last GRU layer\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # 🔥 Swish activation for better gradient flow\n",
        "    Dense(128, activation='swish', kernel_regularizer=l2(0.0005)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='swish', kernel_regularizer=l2(0.0005)),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# ✅ Lower Learning Rate with One-Cycle Policy\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ✅ Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
        "\n",
        "# ✅ Save the best model automatically\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/best_model.keras\",\n",
        "                             monitor=\"val_accuracy\",\n",
        "                             save_best_only=True,\n",
        "                             mode=\"max\")\n",
        "\n",
        "\n",
        "# ✅ Train the Model (Auto-Save)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=70,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop, lr_scheduler, checkpoint]  # Auto-save enabled!\n",
        ")\n",
        "\n",
        "print(\"✅ Model training completed and best model saved!\")\n",
        "\n",
        "# 📊 Training Visualization\n",
        "def plot_training_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', color='red')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='red')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training & Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 📊 Plot the results\n",
        "plot_training_results(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTLBOIM6GU0W",
        "outputId": "363c8c2e-af5d-4252-bff3-117cd62c93c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 1s/step - accuracy: 0.2495 - loss: 2.0486 - val_accuracy: 0.1591 - val_loss: 3.3897 - learning_rate: 3.0000e-04\n",
            "Epoch 2/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 1s/step - accuracy: 0.4253 - loss: 1.5556 - val_accuracy: 0.2817 - val_loss: 2.0525 - learning_rate: 3.0000e-04\n",
            "Epoch 3/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 1s/step - accuracy: 0.5281 - loss: 1.2863 - val_accuracy: 0.3463 - val_loss: 2.0933 - learning_rate: 3.0000e-04\n",
            "Epoch 4/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 1s/step - accuracy: 0.5814 - loss: 1.1093 - val_accuracy: 0.5120 - val_loss: 1.3571 - learning_rate: 3.0000e-04\n",
            "Epoch 5/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 1s/step - accuracy: 0.6193 - loss: 1.0165 - val_accuracy: 0.6376 - val_loss: 0.9675 - learning_rate: 3.0000e-04\n",
            "Epoch 6/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 1s/step - accuracy: 0.6434 - loss: 0.9576 - val_accuracy: 0.5300 - val_loss: 1.4493 - learning_rate: 3.0000e-04\n",
            "Epoch 7/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 1s/step - accuracy: 0.6704 - loss: 0.8975 - val_accuracy: 0.6430 - val_loss: 1.0071 - learning_rate: 3.0000e-04\n",
            "Epoch 8/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.6947 - loss: 0.8411 - val_accuracy: 0.7221 - val_loss: 0.7479 - learning_rate: 3.0000e-04\n",
            "Epoch 9/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 1s/step - accuracy: 0.7087 - loss: 0.8065 - val_accuracy: 0.7124 - val_loss: 0.7836 - learning_rate: 3.0000e-04\n",
            "Epoch 10/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 1s/step - accuracy: 0.7189 - loss: 0.7682 - val_accuracy: 0.6889 - val_loss: 0.8024 - learning_rate: 3.0000e-04\n",
            "Epoch 11/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 1s/step - accuracy: 0.7291 - loss: 0.7367 - val_accuracy: 0.6660 - val_loss: 0.9947 - learning_rate: 3.0000e-04\n",
            "Epoch 12/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.7504 - loss: 0.6886 - val_accuracy: 0.7544 - val_loss: 0.6759 - learning_rate: 1.5000e-04\n",
            "Epoch 13/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 1s/step - accuracy: 0.7640 - loss: 0.6604 - val_accuracy: 0.6910 - val_loss: 1.0160 - learning_rate: 1.5000e-04\n",
            "Epoch 14/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 1s/step - accuracy: 0.7685 - loss: 0.6448 - val_accuracy: 0.7007 - val_loss: 0.8523 - learning_rate: 1.5000e-04\n",
            "Epoch 15/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 1s/step - accuracy: 0.7740 - loss: 0.6291 - val_accuracy: 0.7620 - val_loss: 0.6295 - learning_rate: 1.5000e-04\n",
            "Epoch 16/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 1s/step - accuracy: 0.7804 - loss: 0.6108 - val_accuracy: 0.6836 - val_loss: 0.9970 - learning_rate: 1.5000e-04\n",
            "Epoch 17/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 1s/step - accuracy: 0.7848 - loss: 0.6028 - val_accuracy: 0.7712 - val_loss: 0.6376 - learning_rate: 1.5000e-04\n",
            "Epoch 18/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.7880 - loss: 0.5876 - val_accuracy: 0.7694 - val_loss: 0.6335 - learning_rate: 1.5000e-04\n",
            "Epoch 19/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 1s/step - accuracy: 0.7927 - loss: 0.5753 - val_accuracy: 0.7898 - val_loss: 0.5689 - learning_rate: 7.5000e-05\n",
            "Epoch 20/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.7992 - loss: 0.5644 - val_accuracy: 0.7922 - val_loss: 0.5627 - learning_rate: 7.5000e-05\n",
            "Epoch 21/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 1s/step - accuracy: 0.8048 - loss: 0.5428 - val_accuracy: 0.7539 - val_loss: 0.7467 - learning_rate: 7.5000e-05\n",
            "Epoch 22/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.8068 - loss: 0.5435 - val_accuracy: 0.7638 - val_loss: 0.7100 - learning_rate: 7.5000e-05\n",
            "Epoch 23/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 1s/step - accuracy: 0.8026 - loss: 0.5415 - val_accuracy: 0.7948 - val_loss: 0.5737 - learning_rate: 7.5000e-05\n",
            "Epoch 24/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 1s/step - accuracy: 0.8112 - loss: 0.5283 - val_accuracy: 0.7941 - val_loss: 0.5763 - learning_rate: 3.7500e-05\n",
            "Epoch 25/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 1s/step - accuracy: 0.8113 - loss: 0.5255 - val_accuracy: 0.7929 - val_loss: 0.5825 - learning_rate: 3.7500e-05\n",
            "Epoch 26/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 1s/step - accuracy: 0.8161 - loss: 0.5180 - val_accuracy: 0.7929 - val_loss: 0.5972 - learning_rate: 3.7500e-05\n",
            "Epoch 27/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 1s/step - accuracy: 0.8163 - loss: 0.5120 - val_accuracy: 0.7985 - val_loss: 0.5572 - learning_rate: 1.8750e-05\n",
            "Epoch 28/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 1s/step - accuracy: 0.8188 - loss: 0.5028 - val_accuracy: 0.7951 - val_loss: 0.5750 - learning_rate: 1.8750e-05\n",
            "Epoch 29/70\n",
            "\u001b[1m390/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 1s/step - accuracy: 0.8240 - loss: 0.4999 - val_accuracy: 0.8043 - val_loss: 0.5464 - learning_rate: 1.8750e-05\n",
            "Epoch 30/70\n",
            "\u001b[1m277/390\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 1s/step - accuracy: 0.8196 - loss: 0.5022"
          ]
        }
      ]
    }
  ]
}